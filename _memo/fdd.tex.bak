\part{Fouille de donnée}
\pagebreak

\chapter{Rappel}
\section{Probabilités}
\begin{description}
\item[Quelques rappels de probabilités]: Soient X et Y deux variables aléatoires discrètes prenant leurs valeurs dans DX={x1,..,xn} et DY={y1,..,ym} respectivement.  
\item[$P(x_i)$] = $\frac{|x_i|}{\sum_{j=1}^n |x_j|}$
\item[] $\sum_{i=1}^n P(x_i) = 1$
\item[$P(x_i | y_i)$] = $\frac{P(x_i,y_i)}{p(y_i)}$
\item[$P(x_i,y_i)$] = $p(x_i) * p(y_i)$ Si X et Y sont indépendantes
\item[règle de bayes] = $P(x_i|y_i) = \frac{P(y_i|x_i)*p(x_i)}{p(y_i)}$
\item[règle de chainage $P(x_1,x_2,x_3,...x_n$] = $p(x_1)*p(x_2|x_1)*...*p(x_n|x_{n-1}..x_1)$
\item[distribution conditionnel] $ \forall x \in X, \forall y \in Y => P(x|y)$
\end{description}

Exemple:
\begin{description}
\item[]: 
\begin{tabular}{ll|ll}
  \hline
  Année & Sexe & \# & \%  \\
  \hline
   M1 & M  & 25  &  25/55   \\
   M1 & F  & 4 &  4/55  \\
   M2 & M  & 25  & 25/55   \\
   M2 & F  & 1  &  1/55  \\
  \hline
\end{tabular}
\item[]
\begin{description}
\item[$P(sexe=M)$] = $P(Sexe=M et Annee=M1) + P(Sexe=M et Annee=M2) = 50/55$
\item[$P(Annee=M2 | sexe=M)$] = $P(Sexe=M et Annee=M2) / P(Sexe=M) = \frac{25}{55} / \frac{50}{55} = \frac{25}{50} = \frac{1}{2}$
\end{description}
\end{description}

\section{Exemple}
\begin{multicols}{2}
[]
\begin{tabular}{ll|l}
  \hline
  $A$&$B$&$P(AB)$\\
  \hline
  $a_1$&$b_1$&.1\\
  $a_2$&$b_1$&.15\\
  $a_1$&$b_2$&.3\\
  $a_2$&$b_2$&.45\\
  \hline
\end{tabular}

\begin{itemize}
\item $P(a_1) = .40$
\item $P(a_1|b_1) = .4$
\item $P(a_1|b_2) = .4$
\item $P(a_2) = .60$
\item $P(a_2|b_1) = .6$
\item $P(a_2|b_2) = .6$
\end{itemize}

\end{multicols}

\section{Logarithmes en base 2}
$Log_2(\frac{x}{y}) = Log_2(x) - Log_2(y)$\\
$Log_2(x*y) = Log_2(x) + Log_2(y)$\\

\chapter{Pré traitement des données}
\section{Nettoyage des données}
\subsection{Caractéristiques descriptives}

\begin{description}
\item[Moyenne (espérance)]: $\overset{\_}{x}=\frac{1}{n} \sum_{i=1}^n x_i$
\item[Ecart moyen]: $\frac{1}{n} \sum_{i=1}^n |x_i - \overset{\_}{x} |$
\item[Variance]: $v = \frac{1}{n} \sum_{i=1}^n (x_i - \overset{\_}{x} )^2$
\item[Ecart type]: $\alpha x := \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overset{\_}{x} )^2} = \sqrt{\frac{1}{n}(\sum_{i=1}^n x_i^2) - \overset{\_}{x}^2 }$
\item[Médiane]: Valeur se trouvant au milieu de données ordonnées
\item[Mode]:Valeur la plus fréquente 
\item[Amplitude]:min, max
\end{description}

\section{Normalisation}

\begin{description}
\item[Min-max]: $v_n = \frac{v-v_{min}}{v_{max} - v_{min}}$
\item[Min-max dans l'intervalle [A,B]]: $v_n = \frac{v-v_{min}}{v_{max} - v_{min}} * (B-A) + A$
\item[Z-Score]: $v_n = \frac{v - moyenne}{ecart_type}$
\item[Decimal scaling]: $v_n = \frac{v}{100^j}$
\end{description}

\pagebreak
\chapter{Classification}
\section{Évaluation des classifieurs}
\subsection{Matrice de confusion}
\begin{description}
\item[Percent of correct classification]:
\begin{description}
\item[PCC(\%)]: $ = \frac{N_c}{N_t} * 100$
\item[$N_c$]: nombre d'instances correctement classées
\item[$N_t$]: nombre d'instances testées $(N_t = |D_{test}|)$
\end{description} 
\end{description}

Exemple:\\
\begin{description}
\item[]: $
  \begin{tabular}{l|llll}
  \hline
  \_  & c1 & c2 & c3 & c4 \\
  \hline
   c1 & 0  & 1  &  0 & 0  \\
   c2 & 1  & 60 &  0 & 1  \\
   c3 & 0  & 1  & 23 & 0  \\
   c4 & 1  & 0  &  7 & 5  \\
  \hline
  \end{tabular}$
\item[Taux d'erreurs]: 100-PCC
\item[PCC(\%)] = $ \frac{0+60+23+5}{100} * 100 = 88\% $ 
\item[Coût d'erreur] = $\sum_1^n cout(class_{reelle}, classe_{predite})$
\item[coût d'erreur moyen] = $\frac{cout derreur}{N_{erreurs}}$
\item[$Rappel(C_i)$] = $\frac{N_{c\_i}}{N_{t\_i}} * 100$ $\ (Horizontal)\ Ex:$ $Rappel(C_3)=(23/24)\%$
\item[$Precision(C_i)$] = $\frac{N_{c\_i}}{N_{i}} * 100$ $\ (Vertical)\ Ex:$ $Precision(C_3)=(23/30)\%$
\end{description}


\chapter{Arbre de décision}
\section{critères de sélection C4.5}

Construction d’un arbre de décision C4.5  La construction d'un arbre de décision avec C4.5 passe par deux phases:  
\begin{description}
\item[Phase d'expansion]: La construction se fait selon l'approche descendante et laisse croître l'arbre jusqu'à sa taille maximale.  
\item[Phase d'élagage]: Pour optimiser la taille l'arbre et son pouvoir de généralisation, C4.5 procède à l'élagage (pour supprimer les sous-arbres qui ne minimisent pas le taux d'erreurs)
\item[Approche de construction d’un AD]: Partitionner récursivement les données en sous-ensembles plus homogènes  … jusqu’à obtenir des partitions qui contiennent des objets qui appartiennent majoritairement à la même classe. \\
=> Théorie de l’information pour caractériser le degré de mélange, homogénéité, impureté, incertitude…
\item[Théorie de l’information]: Théorie mathématique ayant pour objet l’étude du contenu informationnel d’un message. \\
 Applications en codage, compression, sécurité… 
\item[Entropie]: Mesure la quantité d’incertitude dans une distribution de probabilités.
\end{description}

\subsection{Entropie}
\begin{description}
\item[Entropie]: Mesure la quantité d’incertitude (manque d’information) dans une distribution de probabilités.   Soit X une variable aléatoire discrète prenant ses valeurs dans $DX={x1,..,xn}$. Soit P la distribution de probabilités associée à X.   
\item[$H(X)$] = $- \sum_{i=1}^n p(x_i) * log_2(p(x_i))$
\item[] Par convention, quand $p(x) = 0, 0*log(0) = 0$
\end{description}

Exemple:
\begin{description}
\item[] $\begin{tabular}{|l|c|r|}
  \hline
   X & P(X)\\
  \hline
  x\_1 & 1/3 \\
  x\_2 & 1/3 \\
  x\_3 & 1/3 \\
  \hline
\end{tabular}$
\item[$H(X)$] = $-p(x_1)*log_2(p(x_1))-p(x_2)*log_2(p(x_2))-p(x_3)*log_2(p(x_3))$
\item[$H(X)$] = $-3(\frac{1}{3}*log_2(\frac{1}{3})) = log_2(3) = 1.58$
\end{description}

Autre exemples:
\begin{description}
\item[] $[\frac{1}{2}, \frac{1}{4}, \frac{1}{4}]: H(X) = 1.5$
\item[] $[1, 0, 0]: H(X) = 0$
\item[] $[\frac{1}{2}, \frac{1}{2}]: H(X) = 1$
\end{description}

Propriétés:
\begin{description}
\item[$H(X)$] $>=$ 0
\item[$H(X)$] est maximale pour une distribution uniforme (toutes les valeurs sont équiprobables).
\end{description}

\begin{description}
\item[Entropie conjointe]: L’entropie conjointe de deux variables aléatoires X et Y est l'incertitude relative à ces deux variables conjointement.
\item[$Entropie(X,Y)$] = $- \sum_{i,j=1}^n p(x_i,y_i) * log_2(p(x_i,y_i))$
\item[Exemple]: $[0.2, 0.1, 0.3, 0.4]: H(X,Y) = 1.85$
\end{description}


\subsection{Gain d'information}
Soit le data suivant, avec ClientSatisfait la variable de classe:\\
\begin{tabular}{lll|l}
\hline
Mémoire & AutonomieBatterie & Prix & ClientSatisfait\\
\hline
$<=4$ & longue & $<=150$ & Oui \\
$>4$ & longue & $>150$ & Oui \\
$>4$ & longue & $<=150$ & Oui \\
$<=4$ & longue & $>150$ & Oui \\
$>4$ & longue & $>150$ & Oui \\
$>4$ & courte & $>150$ & Oui \\
$<=4$ & courte & $>150$ & Non \\
$<=4$ & courte & $>150$ & Non \\
$>4$ & courte & $<=150$ & Oui \\
$<=4$ & courte & $<=150$ & Non \\
$<=4$ & moyen & $<=150$ & Non \\
$>4$ & moyen & $<=150$ & Non \\
$<=4$ & moyen & $>150$ & Oui \\
$>4$ & moyen & $>150$ & Oui \\
$>4$ & moyen & $<=150$ & Non \\
\hline
\end{tabular}
\ \\
Le $Gain\ information$ appliqué sur la colonne AutonomieBatterie (AB) serait:\\
\begin{description}
\item[$Gain(AB)$] $ =\ Entropie(AB) - \frac{5}{15}\ Entropie(Longue) - \frac{5}{15}\ Entropie(Courte) - \frac{5}{15}\ Entropie(Moyen)$
\item[$Entropie(AB)$] $ = -3 (\frac{5}{15} * log_2 (\frac{5}{15} ))$
\item[$Entropie(Longue)$] $= 0$
\item[$Entropie(Courte)$] $= \frac{2}{5} * log_2(\frac{2}{5}) - \frac{3}{5} log_2(\frac{3}{5})$
\item[$Entropie(Moyen)$] $= \frac{3}{5} log_2(\frac{3}{5}) - \frac{2}{5} * log_2(\frac{2}{5})$
\end{description}

\subsection{Gain Ratio}
\cformular{$Gainratio(AB)$}{$= \frac{Gain(AB)}{Entropie(AB)}$}

\section{critères d'arrêt}

\subsection{Critères d'arrêt}
\begin{description}
\item[] Si tout les objets d'une partition appartiennent à une même classes
\item[] Si il n'y a plus aucun attributs à tester
\item[] si le nœud est vide (càd feuille de l'arbre)
\item[] Absence d'apport informationnel (le grain est négatif ou nul)
\end{description}

\subsection{critères d'arrêt: Paramètre utilisateur}
\begin{description}
\item[] Nombre d'objets minimum par feuille
\item[] Taille, profondeur de l'arbre
\item[] Temps de construction de l'arbre
\end{description}

\chapter{Réseau bayésiens}
\section{Classifieur bayésiens}
\scalebox{0.7}{
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=8cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,draw=none,text=black]

  \node[state]		   (A)                    {
    \begin{tabular}{ll}
      $ Nuage $ & $p(N)$\\
      \hline
      \hline N=$\bot$ & N=$\top$\\
      \hline 0.5 & 0.5\\
	\end{tabular}\\
  };
  \node[state]         (B) [below left of=A]  {      
     \begin{tabular}{l|ll}
         $ Arroseur$ & $P(A|N) $& $ $ \\
        \hline
        \hline N & A=$\bot$ & A=$\top$\\
        \hline $\bot$ & 0.8 & 0.2\\
         $\top$ & 0.2 & 0.8\\
	  \end{tabular}\\
  };
  \node[state]         (C) [below right of=A] {     
  	\begin{tabular}{l|ll}
        $ Pluie$ & $ P(P|N)$ & $ $ \\
        \hline
        \hline N & P=$\bot$ & P=$\top$\\
        \hline $\bot$ & 0.99 & 0.1\\
        $\top$ & 0.6 & 0.4\\
	  \end{tabular}\\
   };
  \node[state]         (D) [below right of=B] {     
  	\begin{tabular}{ll|ll}
        $ Pelouse$ & $Mouille $ & $ P(M|A,P)$ & $ $ \\
        \hline
        \hline A & P & M=$\bot$ & M=$\top$\\
        \hline $\bot$ & $\bot$ & 0.9 & 0.1\\
        $\bot$ & $\top$ & 0.2 & 0.8\\
        $\top$ & $\bot$ & 0.2 & 0.8\\
        $\top$ & $\top$ & 0.05 & 0.95\\
	  \end{tabular}\\
   };

  \path (A) edge              node {} (B)
            edge              node {} (C)
        (B) edge              node {} (D)
        (C) edge              node {} (D);
\end{tikzpicture}
}
\begin{description}
\item[Calculer] $P(N=\top,P=\top,A=\bot,M=\top)$
\item[$=$] $P(N=\top) * P(P=\top|N=\top) * P(A=\bot|N=\top,P=\top) * \linebreak P(M=\top|N=\top,P=\top,A=\bot)$
\item[$=$] $ .5\ *\ .4\ *\ \frac{P(N=\top,P=\top)P(A=\bot)}{P(N=\top,P=\top)}\ *\ \frac{P(N=\top,P=\top,A=\bot)*P(M=\top)}{P(N=\top,P=\top,A=\bot)}$
\item[$=$] $ .5\ *\ .4\ *\ 1\ *\ $
\end{description}

\pagebreak
\section{Construction et classification avec des réseaux Bayésiens}
Soit le jeu de donnée suivant:

\begin{tabular}{l|llll}
  \hline
   $ $ & Couleur & Type & Origine & volée \\
  \hline
  1 & rouge & sport & locale & oui\\
  2 & rouge & sport & locale & non\\
  3 & rouge & sport & locale & oui\\
  4 & jaune & sport & locale & non\\
  5 & jaune & sport & importée & oui\\
  6 & jaune & 4x4 & importée & non\\
  7 & jaune & 4x4 & importée & oui\\
  8 & jaune & 4x4 & locale & non\\
  9 & rouge & 4x4 & importée & non\\
  10 & rouge & sport & importée & oui\\
  \hline
\end{tabular}

\subsection{Construction d'un réseau bayésien naïf}
soit la variable de classe nommé "Volée": 

\scalebox{0.7}{
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=6cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,draw=none,text=black]

  \node[state]		   (A)                    {
    \begin{tabular}{l|ll}
      $ Volee $ & $\top$ & $\bot$\\
      \hline 
      $X$ & 5/10 & 5/10\\
	\end{tabular}\\
  };
  \node[state]         (B) [below left of=A]  {      
     \begin{tabular}{l|ll}
         $ Couleur$ & $\top$& $\bot$ \\
         \hline
         $rouge$& 3/5 & 2/5\\
         $jaune$& 2/5 & 3/5\\
	  \end{tabular}\\
  };
  \node[state]         (C) [below right of=A] {     
  	\begin{tabular}{l|ll}
        $ Type$ & $\top$ & $\bot$ \\
        \hline
        $Sport$& 4/5 & 2/5\\
        $4x4$ & 1/5 & 3/5\\
	  \end{tabular}\\
   };
  \node[state]         (D) [below right of=B] {     
  	\begin{tabular}{l|ll}
        $Origine$ & $\top$ & $\bot$ \\
        \hline
        $Locale$ & 2/5 & 3/5\\
        $Importee$&3/5 & 2/5\\
	  \end{tabular}\\
   };

  \path (A) edge              node {} (B)
            edge              node {} (C)
            edge              node {} (D);
\end{tikzpicture}
}

\subsection{Règle de classification bayésienne}
$classes = max
\begin{cases}
P(Volee=\top | Rouge, 4x4, Importee)\\
P(Volee=\bot | Rouge, 4x4, Importee)
\end{cases}$

\subsection{Règle de décision}
\begin{description}
\item[$P(V|CTO)$] = $P(VCTO)$ car indépendantes
\item[$        $] = $P(C|v)*P(T|V)*P(O|V)*P(V)$
\end{description}

\subsection{Observation de classe}
Avec l'observation suivante (Rouge, 4x4, Importée) la classe associée à cette observation est:
\begin{description}
\item[$P(Volee=Non,Rouge,4x4,Importee)$] = $P(Rouge|Non)*P(4x4|Non)*P(Importee|Non)*P(Non)$
\item[$                               $] = $2/5 * 3/5 * 2/5 * 1/2$
\end{description}
\begin{description}
\item[$P(Volee=Oui,Rouge,4x4,Importee)$] = $P(Rouge|Oui)*P(4x4|Oui)*P(Importee|Oui)*P(Oui)$
\item[$                               $] = $ $
\end{description}

\ \\
Avec l'observation incomplète suivante (Jaune, Sport) la classe associée à cette observation est:
\begin{description}
\item[$P(Volee=Non,Jaune,Sport)$] = $P(Jaune|Non)*P(Sport|Non)* \sum P(\theta|Non)*P(Non)$
\item[$                               $] = $2/5 * 4/5 * 1 * 1/2$
\end{description}
\begin{description}
\item[$P(Volee=Oui,Jaune,Sport)$] = $P(Jaune|Oui)*P(Sport|Oui)*\sum P(\theta|Oui)*P(Oui)$
\item[$                               $] = $ $
\end{description}
\pagebreak

\chapter{Clustering}
\pagebreak