\part{Fouille de donnée}
\pagebreak

\chapter{Rappel sur les probabilisées}

\begin{description}
\item[Quelques rappels de probabilités]: Soient X et Y deux variables aléatoires discrètes prenant leurs valeurs dans DX={x1,..,xn} et DY={y1,..,ym} respectivement.  
\item[$P(x_i)$] = $\frac{|x_i|}{\sum_{j=1}^n |x_j|}$
\item[] $\sum_{i=1}^n P(x_i) = 1$
\item[$P(x_i | y_i)$] = $\frac{P(x_i,y_i)}{p(y_i)}$
\item[$P(x_i,y_i)$] = $p(x_i) * p(y_i)$ Si X et Y sont indépendantes
\item[règle de chainage $P(x_1,x_2,x_3,...x_n$] = $p(x_1)*p(x_2|x_1)*...*p(x_n|x_{n-1}..x_1)$
\item[distribution conditionnel] $ \forall x in X, \forall y in Y => P(x|y)$
\end{description}

Exemple:
\begin{description}
\item[]: $\begin{pmatrix}
  Année & Sexe & \# & \%  \\
   M1 & M  & 25  &  25/55   \\
   M1 & F  & 4 &  4/55  \\
   M2 & M  & 25  & 25/55   \\
   M2 & F  & 1  &  1/55  \\
\end{pmatrix}$
\item[]
\begin{description}
\item[$P(sexe=M)$] = $P(Sexe=M et Année=M1) + P(Sexe=M et Année=M2) = 50/55$
\item[$P(Année=M2 | sexe=M)$] = $P(Sexe=M et Année=M2) / P(Sexe=M) = \frac{25}{55} / \frac{50}{55} = \frac{25}{50} = \frac{1}{2}$
\end{description}
\end{description}

\chapter{Pré traitement des données}
\section{Nettoyage des données}
\subsection{Caractéristiques descriptives}

Objectifs: Résumer, décrire certains aspects (tendances, variation, dispersion…) des données en utilisant certaines mesures :

\begin{center}
\begin{description}
\item[Moyenne (espérance)]: $\overset{\_}{x}=\frac{1}{n} \sum_{i=1}^n x_i$
\item[Ecart moyen]: $\frac{1}{n} \sum_{i=1}^n |x_i - \overset{\_}{x} |$
\item[Variance]: $v = \frac{1}{n} \sum_{i=1}^n (x_i - \overset{\_}{x} )^2$
\item[Ecart type]: $\alpha x := \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overset{\_}{x} )^2} = \sqrt{\frac{1}{n}(\sum_{i=1}^n x_i^2) - \overset{\_}{x}^2 }$
\item[Médiane]: Valeur se trouvant au milieu d’une série de données ordonnées
\item[Mode]:Valeur la plus fréquente 
\item[Amplitude]:min, max
\end{description}
\end{center}

\section{Normalisation}

\begin{description}
\item[Min-max]: $v_n = \frac{v-v_{min}}{v_{max} - v_{min}}$
\item[Min-max dans l'intervalle [A,B]]: $v_n = \frac{v-v_{min}}{v_{max} - v_{min}} * (B-A) + A$
\item[Z-Score]: $v_n = \frac{v - moyenne}{ecart_type}$
\item[Decimal scaling]: $v_n = \frac{v}{100^j}$
\end{description}

\pagebreak
\chapter{Classification}
\section{Évaluation des classifieurs}
\subsection{Matrice de confusion}
\begin{description}
\item[Percent of correct classification]:
\begin{description}
\item[PCC(\%)]: $ = \frac{N_c}{N_t} * 100$
\item[$N_c$]: nombre d'instances correctement classées
\item[$N_t$]: nombre d'instances testées $(N_t = |D_{test}|)$
\end{description} 
\end{description}

Exemple:\\
\begin{description}
\item[]: $\begin{pmatrix}
  \_  & c1 & c2 & c3 & c4 \\
   c1 & 0  & 1  &  0 & 0  \\
   c2 & 1  & 60 &  0 & 1  \\
   c3 & 0  & 1  & 23 & 0  \\
   c4 & 1  & 0  &  7 & 5  \\
\end{pmatrix}$
\item[Taux d'erreurs]: 100-PCC
\item[PCC(\%)] = $ \frac{0+60+23+5}{100} * 100 = 88\% $ 
\end{description}

\chapter{Arbre de décision}
\section{critères de sélection C4.5}

Construction d’un arbre de décision C4.5  La construction d'un arbre de décision avec C4.5 passe par deux phases:  
\begin{description}
\item[Phase d'expansion]: La construction se fait selon l'approche descendante et laisse croître l'arbre jusqu'à sa taille maximale.  
\item[Phase d'élagage]: Pour optimiser la taille l'arbre et son pouvoir de généralisation, C4.5 procède à l'élagage (pour supprimer les sous-arbres qui ne minimisent pas le taux d'erreurs)
\item[Approche de construction d’un AD]: Partitionner récursivement les données en sous-ensembles plus homogènes  … jusqu’à obtenir des partitions qui contiennent des objets qui appartiennent majoritairement à la même classe. \\
=> Théorie de l’information pour caractériser le degré de mélange, homogénéité, impureté, incertitude…
\item[Théorie de l’information]: Théorie mathématique ayant pour objet l’étude du contenu informationnel d’un message. \\
 Applications en codage, compression, sécurité… 
\item[Entropie]: Mesure la quantité d’incertitude dans une distribution de probabilités.
\end{description}

\subsection{Entropie}
\begin{description}
\item[Entropie]: Mesure la quantité d’incertitude (manque d’information) dans une distribution de probabilités.   Soit X une variable aléatoire discrète prenant ses valeurs dans $DX={x1,..,xn}$. Soit P la distribution de probabilités associée à X.   
\item[$H(X)$] = $- \sum_{i=1}^n p(x_i) * log_2(p(x_i))$
\item[] Par convention, quand $p(x) = 0, 0*log(0) = 0$
\end{description}

Exemple:
\begin{description}
\item[] $\begin{tabular}{|l|c|r|}
  \hline
   X & P(X)\\
  \hline
  x\_1 & 1/3 \\
  x\_2 & 1/3 \\
  x\_3 & 1/3 \\
  \hline
\end{tabular}$
\item[$H(X)$] = $-p(x_1)*log_2(p(x_1))-p(x_2)*log_2(p(x_2))-p(x_3)*log_2(p(x_3))$
\item[$H(X)$] = $-3(\frac{1}{3}*log_2(\frac{1}{3})) = log_2(3) = 1.58$
\end{description}

Autre exemples:
\begin{description}
\item[] $[\frac{1}{2}, \frac{1}{4}, \frac{1}{4}]: H(X) = 1.5$
\item[] $[1, 0, 0]: H(X) = 0$
\item[] $[\frac{1}{2}, \frac{1}{2}]: H(X) = 1$
\end{description}

Propriétés:
\begin{description}
\item[$H(X)$] >= 0
\item[$H(X)$] est maximale pour une distribution uniforme (toutes les valeurs sont équiprobables).
\end{description}

\begin{description}
\item[Entropie conjointe]: L’entropie conjointe de deux variables aléatoires X et Y est l’incertitude relative à ces deux variables conjointement.
\item[$H(X,Y)$] = $- \sum_{i,j=1}^n p(x_i,y_i) * log_2(p(x_i,y_i))$
\item[Exemple]: $[0.2, 0.1, 0.3, 0.4]: H(X,Y) = 1.85$
\end{description}


Critère de sélection: Gain d'information:
\begin{description}
\item[$GAIN(T,A)$] = $Info(T) - Info(T|A)$
\item[Avec $Info(T)$]: Entropie au niveau de T (avant de partitionner)
\item[$Info(T)$] = $-\sum_{c_i} freq(c_i,T)*log_2(freq(c_i,T))$
\item[Avec $freq(c_i,T)$] = $p(c_i) = \frac{|c_i|}{|T|}$
\item[Avec $Info(T|A)$] l'entropie conditionnelle de T une fois partitionné selon les valeurs de l'attribut A.
\item[$Info(T|A)$] = $ \sum_{a_{j \in A}} freq(a_j,T) * Info(T | a_j)$
\end{description}

Critère de sélection: Gain Ration:
\begin{description}
\item[] Le gain d’information favorise les attributs ayant de larges domaines.
\item[] Le ratio de gain utilise le gain d’information avec un facteur pénalisant les attributs ayant des domaines trop larges.
\item[$GainRatio(T,A)$] = $\frac{Gain(T,A)}{Split_Info(T,A)}$
\item[Avec $Split_Info(T,A)$] = $- \sum_{a_{j \in A}} freq(a_j,T)*log_2(freq(a_j,T)) = $Entropie de A.
\end{description}

\section{critères d'arrêt}

\subsection{Critères d'arrêt}
\begin{description}
\item[] Si tout les objets d'une partition appartiennent à une même classes
\item[] Si il n'y a plus aucun attributs à tester
\item[] si le nœud est vide (càd feuille de l'arbre)
\item[] Absence d'apport informationnel (le grain est négatif ou nul)
\end{description}

\subsection{critères d'arrêt: Paramètre utilisateur}
\begin{description}
\item[] Nombre d'objets minimum par feuille
\item[] Taille, profondeur de l'arbre
\item[] Temps de construction de l'arbre
\end{description}

\chapter{Classificateur bayésiens}
\scalebox{0.8}{
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=8cm,
                    semithick]
  \tikzstyle{every state}=[fill=white,draw=none,text=black]

  \node[state]		   (A)                    {
    \begin{tabular}{ll}
      $ Nuage $ & $p(N)$\\
      \hline
      \hline N=$\bot$ & N=$\top$\\
      \hline 0.5 & 0.5\\
	\end{tabular}\\
  };
  \node[state]         (B) [below left of=A]  {      
     \begin{tabular}{l|ll}
         $ Arroseur$ & $P(A|N) $& $ $ \\
        \hline
        \hline N & A=$\bot$ & A=$\top$\\
        \hline $\bot$ & 0.8 & 0.2\\
         $\top$ & 0.2 & 0.8\\
	  \end{tabular}\\
  };
  \node[state]         (C) [below right of=A] {     
  	\begin{tabular}{l|ll}
        $ Pluie$ & $ P(P|N)$ & $ $ \\
        \hline
        \hline N & P=$\bot$ & P=$\top$\\
        \hline $\bot$ & 0.99 & 0.1\\
        $\top$ & 0.6 & 0.4\\
	  \end{tabular}\\
   };
  \node[state]         (D) [below right of=B] {     
  	\begin{tabular}{ll|ll}
        $ Pelouse$ & $Mouille $ & $ P(M|A,P)$ & $ $ \\
        \hline
        \hline A & P & M=$\bot$ & M=$\top$\\
        \hline $\bot$ & $\bot$ & 0.9 & 0.1\\
        $\bot$ & $\top$ & 0.2 & 0.8\\
        $\top$ & $\bot$ & 0.2 & 0.8\\
        $\top$ & $\top$ & 0.05 & 0.95\\
	  \end{tabular}\\
   };

  \path (A) edge              node {} (B)
            edge              node {} (C)
        (B) edge              node {} (D)
        (C) edge              node {} (D);
\end{tikzpicture}
}
\begin{description}
\item[Calculer] $P(N=\top,P=\top,A=\bot,M=\top)$
\item[$=$] $P(N=\top) * P(P=\top|N=\top) * P(A=\bot|N=\top,P=\top) * \linebreak P(M=\top|N=\top,P=\top,A=\bot)$
\item[$=$] $ .5\ *\ .4\ *\ \frac{P(N=\top,P=\top)P(A=\bot)}{P(N=\top,P=\top)}\ *\ \frac{P(N=\top,P=\top,A=\bot)*P(M=\top)}{P(N=\top,P=\top,A=\bot)}$
\item[$=$] $ .5\ *\ .4\ *\ 1\ *\ ?$
\end{description}

\pagebreak