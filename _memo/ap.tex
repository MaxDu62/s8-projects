\part{Apprentissage automatique par la pratique}
\pagebreak

\chapter{Rappel}
\section{Matrices et calcules sur les Matrices}

\subsection{Addition}

$\left(\begin{array}{cc}
1 & 3 \\ 1 & 0 \\ 1 & 2 \end{array} \right)$
+
$\left(\begin{array}{cc}
0 & 0 \\ 7 & 5 \\ 2 & 1 \end{array} \right) $
=
$\left(\begin{array}{cc}
1+0 & 3+0 \\ 1+7 & 0+5 \\ 1+2 & 2+1 \end{array} \right) $
=
$\left(\begin{array}{cc}
1 & 3 \\ 8 & 5 \\ 3 & 3 \end{array} \right) $

\subsection{Multiplication}

$\begin{array}{c@{\ }c}
&
\left(\begin{array}{cc}
5 & 6 \\ 7 & 8 \end{array} \right) \\[0.5cm]
\left(\begin{array}{cc}
1 & 2 \\ 3 & 4 \end{array} \right)
&
\left(\begin{array}{cc}
19 & 22 \\ 43 & 50 \end{array} \right)
\end{array}$

$(1 * 5) + (2 * 7) = 19$

\subsection{Transposer}

$\left(\begin{array}{ccc}
1 & 3 & 5 \\ 2 & 4 & 6 \end{array} \right) $
= 
$\left(\begin{array}{cc}
1 & 2 \\ 3 & 4 \\ 5 & 6 \end{array} \right) $

\subsection{Inverse}
\begin{description}
\item[Soit une matrice 2x2 comme]:
$\left(\begin{array}{ccc}
a & b \\ c & d \end{array} \right) $
\item[Soit Determinant D] = ad - bc
\item[Si D != 0 alors il existe une matrice inverse égal à]:
$ \frac{1}{D} \left(\begin{array}{ccc}
d & -b \\ -c & a \end{array} \right) $
\end{description}

\chapter{Algorithms Learn a Mapping From Input to Output}
\section{linear ML algorithms}

\begin{description}
\item[] Simplifier les processus d'apprentissage et réduire la fonction sur ce qu'on connait
\item[Soit ]: B0 + B1X1 + B2X2 + B3X3 = 0
\item[] Où B0,B1,B2,B3 sont les coefficients présent sur l'axe des ordonnées.
\item[] Et X1,X2,X3 sont les valeurs en Input.
\end{description}

\section{Supervised machine learning}
L'apprentissage supervisé peut se diviser en 2 partis
\begin{description}
\item[Classification]: Quand les variables en sortie sont des Classe $(Vert, Carre, Homme)$
\item[Regression]: Quand les variables en sortie sont des valeur numérique $(euro, poids, quantites)$
\end{description}

\section{Unsupervised machine learning}
Les problèmes de l'apprentissage non supervisé sont:
\begin{description}
\item[Clustering]: L'art de faire des paquet d'éléments qui ont des points commun, comme regrouper les clients par paquet de choses qu'ils ont le plus en commun.
\item[Association]: Associer des règles d'apprentissage pour décrire une portion du data, comme une personne qui a acheté un item A et qui est aussi tenté par acheter un item B
\end{description}

\section{semi-supervised machine leaning}
L'apprentissage semi supervisé c'est avoir un bonne quantité de données en input X, et un peu de data avec le label Y.

\section{Overview of dias and variance}
La prédiction des erreurs pour les algorithmes sont regroupé en 3 points:
\begin{description}
\item[Bias Error]:  Simplifier l'hypothèse fait par le modèls pour faire une fonction d'apprentissage plus facile.
\item[Variance Error]: Et la quantité estimé par la fonction visé qui changera via un différent ensemble de data utilisé.
\item[Irreductible Error]: Ne peut pas être réduit
\end{description}

\chapter{Overfitting and Underfitting}
\section{Overfitting}
L'overfitting intervient lorsque le modèle sur apprend des connaissances,
Lorsque l'on sur apprend nous prenons en compte les points plus éloigné de la droite de la fonction.\\
On peut illustrer l'overfitting en codant un algorithme qui prend en compte les points bleu et rouges de la figure $\textit{ap-linear-regression\_1}$ ce dessous.\\

\section{Underfitting}
C'est l'inverse de l'overfitting, pas assez de données pour pouvoir généraliser le base de connaissance.\\
\pagebreak
\chapter{Linear Algorithms}

Soit X l'ensemble des variables indépendantes sur l'axe des l'abscisse et
Y l'ensemble des variable dépendantes sur l'axe des ordonnée.

\section{Régression linéaire}
Étant donné un plan à deux dimensions où l'abscisse contient les point d'entrée X et l'ordonnée contient les points de sortie Y, et un nouage de points précédaient acquitté de tout point éloigné du nuage.\\

\scalebox{0.7}{
\begin{tikzpicture}
  \begin{axis} [
      xlabel     = X, % label x axis
      ylabel     = Y, % label y axis
      axis lines = left, %set the position of the axes
      clip       = false, 
      xmin       = 0,
      ymin       = 0,
    ]
    \addplot [color=black] coordinates { (0,0)(10,10) };
    \addplot [only marks, mark=*, color=blue] coordinates {(1,2)(2.5,3)(5,6)(4,3)(8,9)(6,4)(3,4)(7,6) };
    \addplot [only marks, mark=*, color=red] coordinates {(1,9)(8,1)(9,1.5) };
  \end{axis}
\end{tikzpicture}
}
\begin{description}
\item[Avec]: y  = $\beta_0 + \beta_1 x$
\item[Pour un hyperPlan (3d)]: y = $\beta_0 + \beta_1 x_1 + \beta_2 x_2$
\item[$P-I_n$]: y = $\beta_0 + \beta_1 x_1 + ... \beta_n x_n$
\end{description}

\pagebreak
\section{Least squares linear regression}
Calculer la régression linéaire avec la méthode Least squares:\\
Soit:
\begin{description}
\item[X] = $[1,2,3,4,5]$ les variables indépendantes d'axe abscisse
\item[Y] = $[2,4,5,4,5]$ les variables dépendantes d'axe ordonnée
\item[Calculons] $y = \beta_0 + \beta_1 x$
\end{description}
Calcule de la moyenne de X et Y:
\begin{description}
\item[Xm] = $ \sum x_i \in X$ = 3
\item[Ym] = $ \sum y_i \in Y$ = 4
\end{description}
Toutes ligne de régression doivent passer par le point (Xm,Ym).\\
Calculer tout les écarts des $x_i \in X$ par rapport à Xm (resp Y):\\

\begin{tabular}{ll|l|l|l|l}
  \hline
  X  & Y & $X - Xm$ & $Y - Ym$ & $(X-Xm)^2$ & $(X-Xm)(Y-Ym)$\\
  \hline
  1 & 2 & -2 & -2 & 4 & 4\\
  2 & 4 & -1 & 0  & 1 & 0\\
  3 & 5 & 0  & 1  & 0 & 0\\
  4 & 4 & 1  & 0  & 1 & 0\\
  5 & 5 & 2  & 1  & 4 & 2\\ 
  \hline
\end{tabular}

\begin{description}
\item[$Calculer \beta_1$]:
\item[$ \beta_1 $] = $ \frac{ \sum (X-Xm)(Y-Ym)}{ \sum (X-Xm)^2}$ = $\frac{6}{10}$ = $.6$
\item[$ \beta_0 $]: $Ym = \beta_0 + \beta_1 * Xm$ : $4 = \beta_0 + .6 * 3$ : $4= \beta_0 + 1.8$ : $\beta_0 = 2.2$
\end{description}
\pagebreak
\section{Gradient Descent}
Soit:
\begin{description}
\item[X] = $[1,2,4,3,5]$
\item[Y] = $[1,3,3,2,5]$
\item[i] = une variable qui itère les éléments de X et Y en bouclant à l'infini.
\end{description}
Une initialisation comme:
\begin{description}
\item[$\beta_0$] = 0
\item[$\beta_1$] = 0
\item[$\alpha$] = donnée en énoncé (pour l'exemple égal à 0.01)
\end{description}
Et des fonctions définit tel que:
\begin{description}
\item[error] = $(\beta_0 + \beta_1 * X[i]) - Y[i]$
\item[$\beta_{0_{+1}}$] = $\beta_0 - \alpha * error$
\item[$\beta_{1_{+1}}$] = $\beta_1 - \alpha * error * X[i]$
\end{description}

En appliquant l'algorithme des calcules des $\beta_i$:\\
\begin{tabular}{l|l|l|l|l|l}
  \hline
  $ i $ & $ X[i] $ &  $Y[i] $ &  $error $ & $ \beta_0 $ & $ \beta_1 $\\
  \hline
  0 & 1 & 1 & -1 & 0.01 & 0.01 \\
  1 & 2 & 3 & -2.97 & 0.06 & 0.03\\
  2 & 4 & 3 & -1.77 & 0.18 & 0.06\\
  3 & 3 & 2 & -1.61 & 0.22 & 0.08\\
  4 & 5 & 5 & -4.35 & 0.44 & 0.12\\
  0 & 1 & 1 & -0.42 & 0.45 & 0.13\\
  1 & 2 & 3 & -2.28 & 0.49 & 0.49\\
  \hline
\end{tabular}
\pagebreak
\chapter{Logistic Regression}
\section{Logistic function}

Soit:
\begin{description}
\item[t] $\in \Re[0,1]$ égal à $\beta_0 + \beta_2 * x$
\end{description}

La fonction de logique de régression, les valeur d'entrée X sont combiné en utilisant les coefficient de valeur pour prédire une sortie Y. Cette sortie sera une valeur binaire.

\begin{description}
\item[$p(x)$] = $ \frac{1}{1 + e^{-(P-I_n)}}$
\item[Note]: $p(x)$ peut être interprété comme une fonction de probabilité $P(X) = P[Y=1 | X)$.
\item[$\beta_0 + \beta_1 * x$] = $ ln(\frac{P(x)}{1 - P(x)})$ aussi appelé odds.
\end{description}

\chapter{Linear Discriminant Analysis}
L'analyse discriminante linéaire fait partie des techniques d'analyse discriminante prédictive, il s'agit de prédire l'appartenance d'un individu à une classe prédéfinie à partir de ses caractéristiques mesurées à l'aide de variables prédictives.\\
A notre disposition, un échantillon de $n$ observations réparties dans $\Bbbk$ groupes d'effectifs $n_{\Bbbk}$.\\
\begin{description}
\item[Noté $Y$] les variables prédire $\{y1, ... y_{\Bbbk}\}$
\item[$J$] variables prédictives $X = (X_1, ... X_j)$
\item[$\mu_{\Bbbk}$] la moyenne (ou $\textit{mean}$ en anglais) valant $lambda(list) -> \frac{\sum list[i]}{taille(list)}$
\item[$\sigma^2$] la variance de toutes les classes $\frac{\sum_{i=1}^n (x_i - \mu_{\Bbbk})^2}{n - \Bbbk}$
\item[la fonction discriminante pour la classe $\Bbbk$ avec $x$ donné] $D_{\Bbbk} (x) = x * \frac{\mu_{\Bbbk}}{\omega^2} - \frac{\mu_{\Bbbk}^2}{2x\omega^2} + ln(P(k))$
\item[Où $P(k)$] vaut la probabilité appliqué aux valeurs de $Y$
\end{description}

\subsection{bayésien rules}
L'objectif est de produire une règle d'affection $X(\omega) \rightarrow Y(\omega)$ qui permet de prédire, pour une observation $\omega$ donné, sa valeur associé de $Y$ à partir des valeurs prises par $X$. via une probabilité\\
\begin{description}
\item[$P(Y=y_{\Bbbk})$] = $\frac{P(Y=y_{Bbbk})*P(X|Y=y_{\Bbbk})}{\sum_{i=1}^{\Bbbk} P(Y=y_i)*P(X|Y=y_i)}$
\item[Où $P(Y=y_{\Bbbk})$] est la probabilité à $priori$ d'appartenance à une classe
\item[$P(X|Y=y_{\Bbbk})$] représente la fonction de densité des X conditionnellement à la classe $y_{\Bbbk}$
\end{description}

\chapter{Non linear algorithm}
\section{Classification and régression tree}
Soit:\\
\begin{description}
\item[$G$] $ = \sum_{k=1}^n p_k * (1 - p_k)$
\item[$V$] = 2.67
\item[] \begin{tabular}{ll|l}
  \hline
  $ X_1 $ & $ X_2 $ &  $Y $\\
  \hline
  2.77 & 2.33 & 0\\
  1.72 & 01.78 & 0\\
  3.67 & 03.36 & 0\\
  3.96 & 4.67 & 0\\
  \hline
\end{tabular}
\end{description}

Soit un arbre de décision ayant comme fils gauche des $Yes$ et fils droit des $No$ par rapport à la condition $split$.\\
Si la valeur $V < X1_i$ alors on crée un fils gauche, sinon on crée un fils droit:\\

\begin{tikzpicture}[sibling distance=8em,
  every node/.style = {scale=1,
    draw=none, align=center}]]
  \node {$V < 2.77$}
 	  child { node {$V < 1.72$ }
 	    child { node {$Class Leaf$}}
 	    child { node {$V < 3.67$}
 	      child { node {$V < 3.96$}
 	        child { node {$Class Leaf$} }
 	        child { node {$Class Leaf$} }
 	      }
 	      child { node {$Class Leaf$} }
 	    }
 	  }
 	  child { node {$Class Leaf$} }
    ;
\end{tikzpicture}
\ \\
\begin{multicols}{2}
[Soit d'une façon plus calculatoire:
]
\begin{description}
\item[$G$] $=$
\item[] $left(X1_1) * (1 - left(X1_1))$ + 
\item[] $right(X1_1) * (1 - right(X1_1))$ + 
\item[] $left(X1_2) * (1 - left(X1_2))$ + 
\item[] $right(X1_2) * (1 - right(X1_2))$ + 
\item[] $left(X1_3) * (1 - left(X1_3))$ + 
\item[] $right(X1_3) * (1 - right(X1_3))$ + 
\item[] $left(X1_4) * (1 - left(X1_4))$ + 
\item[] $right(X1_4) * (1 - right(X1_4))$ + 
\end{description}

\begin{description}
\item[] $ $
\item[] $X1_1 = 2.77$
\item[] $ = 0$ car $V <$ 2.77 $\rightarrow$ Left
\item[] $ = 0$ car $1.72 <$ V $\rightarrow$ Right
\item[] $X1_2 = 1.72$
\item[] $X1_1 = 3.67$
\item[] $ = 0$ car $V <$ 3.67 $\rightarrow$ Left
\item[] $X1_1 = 3.96$
\item[] $ = 0$ car $V <$ 3.96 $\rightarrow$ Left
\end{description}
\end{multicols}
\pagebreak

\section{K moyen}
Le K moyen demande une heuristique de type métrique pour comparé les distances entre poins.\\
Par exemple:
\begin{description}
\item[Distance euclidienne] $\sqrt{\sum_{i=1}^n (a_i, b_i)^2}$
\end{description}

% a remplir

\section{Support vector machines}
Soit les points:
\begin{description}
\item[$\cblue{Blue}$] Une $\cblue{ClassA}$
\item[$\crouge{Rouge}$] Une $\crouge{ClassB}$
\end{description}
Le support vector machines cherche un hyperplan (de couleur noir) pouvant départager les deux classes,
Il en existe une infinité d'hyperplan qui peuvent les départager, donc introduisons un autre concept, celui de 
l'hyperplan qui maximise la séparation entre les deux classes (les droites $\corange{Oranges}$ appelé $Margin$.\\

\begin{tikzpicture}
  \begin{axis} [
      axis lines = left,
      xmin       = 0,
      ymin       = 0,
    ]
    \addplot [color=black] coordinates {(0,0)(10,10)};
    \addplot [color=orange] coordinates {(0,1.3)(8.7,10) };
    \addplot [color=orange] coordinates {(1,0)(10,9) };
    \addplot [only marks, mark=*, color=blue] coordinates {(2,6)(1,3)(6,7.6)(3,8.5)(7.5,9.5)(4,9)(3,6)};
    \addplot [only marks, mark=*, color=red]  coordinates {(2,0.8)(6,2)(9,4)(9.7,8)(8,3)};
  \end{axis}
\end{tikzpicture}

Dans le cadre du Soft margin, il n'existe pas de margin séparent les deux classes,il faut donc chercher la droite qui minimise l'erreur.

\pagebreak